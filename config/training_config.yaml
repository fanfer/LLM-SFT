# 模型配置
model:
  model_name: "Qwen/Qwen-7B-Chat"  # 或者 "Qwen/Qwen2.5-7B-Instruct"
  cache_dir: "./models"
  trust_remote_code: true
  torch_dtype: "bfloat16"
  device_map: "auto"

# LoRA配置
lora:
  r: 64
  lora_alpha: 128
  target_modules: ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
  lora_dropout: 0.1
  bias: "none"
  task_type: "CAUSAL_LM"

# 训练配置
training:
  output_dir: "./outputs"
  num_train_epochs: 3
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4
  gradient_accumulation_steps: 4
  learning_rate: 2e-4
  weight_decay: 0.01
  warmup_ratio: 0.1
  lr_scheduler_type: "cosine"
  logging_steps: 10
  eval_steps: 500
  save_steps: 500
  save_total_limit: 3
  max_seq_length: 2048
  fp16: false
  bf16: true
  gradient_checkpointing: true
  dataloader_pin_memory: false
  remove_unused_columns: false

# 课程学习配置
curriculum_learning:
  enabled: true
  stages:
    - name: "basic_scoring"
      epochs: 1
      tasks: ["competitiveness", "innovation"]
      data_ratio: 0.3
    - name: "intermediate_scoring" 
      epochs: 1
      tasks: ["competitiveness", "innovation", "diversity"]
      data_ratio: 0.6
    - name: "full_scoring"
      epochs: 1
      tasks: ["competitiveness", "innovation", "diversity", "sales_performance", "market_position", "financial_health"]
      data_ratio: 1.0

# 多任务学习配置
multitask:
  task_weights:
    competitiveness: 1.0
    innovation: 1.0
    diversity: 1.0
    sales_performance: 1.0
    market_position: 1.0
    financial_health: 1.0
  loss_combination: "weighted_sum"  # weighted_sum, uncertainty_weighting
  
# 数据配置
data:
  train_data_path: "./data/train"
  eval_data_path: "./data/eval"
  test_data_path: "./data/test"
  max_samples_per_task: 10000
  
# 实验配置
experiment:
  name: "qwen_lora_risk_profiling"
  wandb_project: "risk-profiling-llm"
  seed: 42
  
# 评估配置
evaluation:
  metrics: ["mae", "mse", "pearson_corr", "accuracy_bins"]
  score_bins: [0, 2, 4, 6, 8, 10]  # 0-1, 2-3, 4-5, 6-7, 8-9 区间 