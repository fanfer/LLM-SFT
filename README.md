# å¤§æ¨¡å‹è®­ç»ƒæ¡†æ¶

åŸºäºLoRAå¾®è°ƒQwenæ¨¡å‹çš„ã€‚æ”¯æŒè¯¾ç¨‹å­¦ä¹ å’Œå¤šä»»åŠ¡å­¦ä¹ ã€‚


## ğŸ”§ ç¯å¢ƒé…ç½®

### ç³»ç»Ÿè¦æ±‚
- Python 3.8+
- CUDA 11.8+ (å¦‚æœä½¿ç”¨GPU)
- 8GB+ æ˜¾å­˜ (æ¨è16GBä»¥ä¸Š)

### å®‰è£…ä¾èµ–
```bash
# å…‹éš†é¡¹ç›®
git clone <your-repo-url>
cd lora-sft

# å®‰è£…ä¾èµ–
pip install -r requirements.txt

# å¦‚æœä½¿ç”¨conda
conda create -n risk-profiling python=3.9
conda activate risk-profiling
pip install -r requirements.txt
```

## ğŸ“Š æ•°æ®æ ¼å¼

### è®­ç»ƒæ•°æ®æ ¼å¼
æ¯ä¸ªç»´åº¦çš„æ•°æ®æ–‡ä»¶åº”ä¸ºJSONæ ¼å¼ï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š

```json
[
  {
    "prompt": "è¯·è¿›è¡Œè¯„ä¼°",
    "company_info": "è¯¦ç»†ä¿¡æ¯æè¿°...",
    "think": "åˆ†ææ€è·¯å’Œè¿‡ç¨‹...",
    "score": 7,
    "reason": "è¯„åˆ†ç†ç”±å’Œä¾æ®..."
  }
]
```

### æ•°æ®ç›®å½•ç»“æ„
```
data/
â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ competitiveness.json    
â”‚   â”œâ”€â”€ innovation.json         
â”‚   â”œâ”€â”€ diversity.json          
â”‚   â””â”€â”€ ...
â”œâ”€â”€ eval/
â”‚   â””â”€â”€ ...
â””â”€â”€ test/
    â””â”€â”€ ...
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. è¯¾ç¨‹å­¦ä¹ è®­ç»ƒ
```bash
python train.py --mode curriculum --config config/training_config.yaml
```

### 2. æ•°æ®é…æ¯”å®éªŒ
```bash
python train.py --mode ratio_exp --ratios 0.2 0.4 0.6 0.8 1.0
```

### 3. æ¨¡å‹è¯„ä¼°
```bash
python train.py --mode eval --model_path ./outputs/stage_3_full_scoring
```

## âš™ï¸ é…ç½®è¯´æ˜

### ä¸»è¦é…ç½®æ–‡ä»¶: `config/training_config.yaml`

#### æ¨¡å‹é…ç½®
```yaml
model:
  model_name: "Qwen/Qwen-7B-Chat"  # åŸºç¡€æ¨¡å‹
  cache_dir: "./models"            # æ¨¡å‹ç¼“å­˜ç›®å½•
  torch_dtype: "bfloat16"          # æ•°æ®ç±»å‹
```

#### LoRAé…ç½®
```yaml
lora:
  r: 64                    # LoRA rank
  lora_alpha: 128         # LoRA alpha
  target_modules: [...]   # ç›®æ ‡æ¨¡å—
  lora_dropout: 0.1       # Dropoutç‡
```

#### è¯¾ç¨‹å­¦ä¹ é…ç½®
```yaml
curriculum_learning:
  enabled: true
  stages:
    - name: "basic_scoring"
      epochs: 1
      tasks: ["competitiveness", "innovation"]
      data_ratio: 0.3
    - name: "intermediate_scoring"
      epochs: 1
      tasks: ["competitiveness", "innovation", "diversity"]
      data_ratio: 0.6
    - name: "full_scoring"
      epochs: 1
      tasks: ["competitiveness", "innovation", "diversity", "sales_performance", "market_position", "financial_health"]
      data_ratio: 1.0
```

## ğŸ“ˆ è®­ç»ƒç­–ç•¥

### è¯¾ç¨‹å­¦ä¹  (Curriculum Learning)
é‡‡ç”¨æ¸è¿›å¼è®­ç»ƒç­–ç•¥ï¼š
1. **åŸºç¡€é˜¶æ®µ**: å…ˆè®­ç»ƒæ ¸å¿ƒç»´åº¦
2. **ä¸­çº§é˜¶æ®µ**: å¢åŠ æ›´å¤šç»´åº¦ï¼Œæ‰©å¤§æ•°æ®è§„æ¨¡
3. **é«˜çº§é˜¶æ®µ**: ä½¿ç”¨å…¨éƒ¨ç»´åº¦å’Œå®Œæ•´æ•°æ®

### å¤šä»»åŠ¡å­¦ä¹  (Multi-task Learning)
- åŒæ—¶åœ¨å¤šä¸ªè¯„ä¼°ç»´åº¦ä¸Šè®­ç»ƒ
- å…±äº«åº•å±‚ç‰¹å¾è¡¨ç¤º
- æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›

### æ•°æ®é…æ¯”å®éªŒ
- æµ‹è¯•ä¸åŒæ•°æ®é‡å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“
- æ‰¾åˆ°æœ€ä¼˜çš„æ•°æ®é…æ¯”
- æ”¯æŒæˆæœ¬æ•ˆç›Šåˆ†æ

## ğŸ” è¯„ä¼°æŒ‡æ ‡

### è¯„åˆ†å‡†ç¡®æ€§
- **MAE (å¹³å‡ç»å¯¹è¯¯å·®)**: é¢„æµ‹è¯„åˆ†ä¸çœŸå®è¯„åˆ†çš„å¹³å‡ç»å¯¹å·®å€¼
- **RMSE (å‡æ–¹æ ¹è¯¯å·®)**: é¢„æµ‹è¯¯å·®çš„å‡æ–¹æ ¹
- **çš®å°”é€Šç›¸å…³ç³»æ•°**: é¢„æµ‹å€¼ä¸çœŸå®å€¼çš„çº¿æ€§ç›¸å…³æ€§
- **åˆ†ç®±å‡†ç¡®æ€§**: æŒ‰è¯„åˆ†åŒºé—´çš„åˆ†ç±»å‡†ç¡®æ€§

### æ¨ç†è´¨é‡
- **é€»è¾‘è¿è´¯æ€§**: æ¨ç†è¿‡ç¨‹çš„é€»è¾‘æ€§
- **ä¸“ä¸šæœ¯è¯­ä½¿ç”¨**: ä¸“ä¸šè¯æ±‡çš„ä½¿ç”¨æƒ…å†µ
- **è®ºè¯å……åˆ†æ€§**: æ¨ç†å†…å®¹çš„å®Œæ•´æ€§
- **å…·ä½“æ€§**: æ˜¯å¦åŒ…å«å…·ä½“æ•°æ®å’Œä¾‹å­

### ä¸€è‡´æ€§æŒ‡æ ‡
- **ç»´åº¦é—´ç›¸å…³æ€§**: ä¸åŒç»´åº¦è¯„åˆ†çš„ç›¸å…³æ€§
- **è¯„åˆ†æ–¹å·®**: è¯„åˆ†çš„ç¨³å®šæ€§
- **è¯„åˆ†åˆ†å¸ƒ**: è¯„åˆ†çš„å¤šæ ·æ€§å’Œåˆ†å¸ƒæƒ…å†µ

## ğŸ“ é¡¹ç›®ç»“æ„

```
lora-sft/
â”œâ”€â”€ config/
â”‚   â””â”€â”€ training_config.yaml    # è®­ç»ƒé…ç½®
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_processor.py       # æ•°æ®å¤„ç†æ¨¡å—
â”‚   â”œâ”€â”€ model_framework.py      # æ¨¡å‹æ¡†æ¶
â”‚   â””â”€â”€ evaluation.py           # è¯„ä¼°æ¨¡å—
â”œâ”€â”€ data/                       # æ•°æ®ç›®å½•
â”œâ”€â”€ outputs/                    # è®­ç»ƒè¾“å‡º
â”œâ”€â”€ train.py                    # ä¸»è®­ç»ƒè„šæœ¬
â”œâ”€â”€ requirements.txt            # ä¾èµ–åˆ—è¡¨
â””â”€â”€ README.md                   # é¡¹ç›®è¯´æ˜
```

## ğŸ’¡ é«˜çº§ç”¨æ³•

### è‡ªå®šä¹‰ç»´åº¦
åœ¨ `src/data_processor.py` ä¸­æ·»åŠ æ–°çš„è¯„ä¼°ç»´åº¦ï¼š

```python
self.all_dimensions = {
    "competitiveness": "ç«äº‰åŠ›",
    "innovation": "åˆ›æ–°æ€§",
    "your_custom_dimension": "è‡ªå®šä¹‰ç»´åº¦",  # æ·»åŠ è¿™é‡Œ
    # ...
}
```

### æ¨¡å‹æ¨ç†
```python
from src.model_framework import MultiTaskLoRAModel
from src.data_processor import RiskProfilingDataProcessor

# åŠ è½½æ¨¡å‹
model = MultiTaskLoRAModel(config)
model.load_model("path/to/trained/model")

# åˆ›å»ºprompt
processor = RiskProfilingDataProcessor(model.tokenizer)
prompt = processor.create_multitask_prompt(
    company_info="ä¿¡æ¯...", 
    dimensions=["competitiveness", "innovation"],
    mode="inference"
)

# ç”Ÿæˆè¯„ä¼°
response = model.generate_response(prompt)
print(response)
```

## ğŸ› æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

**1. æ˜¾å­˜ä¸è¶³**
- å‡å°‘ `per_device_train_batch_size`
- å¯ç”¨ `gradient_checkpointing`
- ä½¿ç”¨ `load_in_8bit` é‡åŒ–

**2. è®­ç»ƒé€Ÿåº¦æ…¢**
- å¢åŠ  `gradient_accumulation_steps`
- ä½¿ç”¨æ›´å¤§çš„batch size
- å¯ç”¨flash attention

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ MIT è®¸å¯è¯ï¼Œè¯¦è§ LICENSE æ–‡ä»¶ã€‚
